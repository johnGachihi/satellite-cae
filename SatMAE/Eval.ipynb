{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "215e29da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from typing import List, Tuple, Optional, Union\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import json\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "import rasterio\n",
    "import random\n",
    "from glob import glob\n",
    "import math\n",
    "\n",
    "from util.datasets import SentinelIndividualImageDataset, SentinelNormalize\n",
    "from util.misc import load_model\n",
    "from models_cae import cae_vit_base_patch16\n",
    "from models_mae import mae_vit_base_patch16\n",
    "import util.misc as misc\n",
    "\n",
    "device = torch.cuda.is_available() and 'cuda' or 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ac2ca964",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_cae(model: torch.nn.Module,\n",
    "         data_loader,\n",
    "         device: torch.device,\n",
    "         args=None):\n",
    "    \"\"\"\n",
    "    Run model evaluation on test dataset.\n",
    "    Args:\n",
    "        model: The model to evaluate\n",
    "        data_loader: DataLoader containing test data\n",
    "        device: Device to run testing on\n",
    "        args: Arguments containing mask_ratio\n",
    "    Returns:\n",
    "        dict: Dictionary containing average test losses\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    metric_logger = misc.MetricLogger(delimiter=\"  \")\n",
    "    header = 'Test:'\n",
    "\n",
    "    # Collect all losses for final statistics\n",
    "    all_losses = []\n",
    "    all_losses_main = []\n",
    "    all_losses_align = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for samples, *_ in metric_logger.log_every(data_loader, 20, header):\n",
    "            samples = samples.to(device, non_blocking=True)\n",
    "            \n",
    "            with torch.cuda.amp.autocast():\n",
    "                loss, loss_main, loss_align, *_ = model(samples, mask_ratio=0.75)\n",
    "            \n",
    "            loss_value = loss.item()\n",
    "            loss_main_value = loss_main.item()\n",
    "            loss_align_value = loss_align.item()\n",
    "            \n",
    "            \n",
    "            if not math.isfinite(loss_value):\n",
    "                print(\"Loss is {}, stopping testing\".format(loss_value))\n",
    "                raise ValueError(f\"Loss is {loss_value}, stopping testing\")\n",
    "           \n",
    "            # Collect losses\n",
    "            all_losses.append(loss_value)\n",
    "            all_losses_main.append(loss_main_value)\n",
    "            all_losses_align.append(loss_align_value)\n",
    "            \n",
    "            # Update metrics\n",
    "            metric_logger.update(loss=loss_value)\n",
    "            metric_logger.update(loss_main=loss_main_value)\n",
    "            metric_logger.update(loss_align=loss_align_value)\n",
    "\n",
    "    # Calculate final statistics\n",
    "    avg_loss = sum(all_losses) / len(all_losses)\n",
    "    avg_loss_main = sum(all_losses_main) / len(all_losses_main)\n",
    "    avg_loss_align = sum(all_losses_align) / len(all_losses_align)\n",
    "\n",
    "    # Print results\n",
    "    print('=' * 80)\n",
    "    print(f'Test Results:')\n",
    "    print(f'Average Loss: {avg_loss:.4f}')\n",
    "    print(f'Average Main Loss: {avg_loss_main:.4f}')\n",
    "    print(f'Average Align Loss: {avg_loss_align:.4f}')\n",
    "    print('=' * 80)\n",
    "\n",
    "    return {\n",
    "        'test_loss': avg_loss,\n",
    "        'test_loss_main': avg_loss_main,\n",
    "        'test_loss_align': avg_loss_align\n",
    "    }\n",
    "\n",
    "\n",
    "def test_mae(model: torch.nn.Module,\n",
    "         data_loader,\n",
    "         device: torch.device,\n",
    "         args=None):\n",
    "    \"\"\"\n",
    "    Run model evaluation on test dataset.\n",
    "    Args:\n",
    "        model: The model to evaluate\n",
    "        data_loader: DataLoader containing test data\n",
    "        device: Device to run testing on\n",
    "        args: Arguments containing mask_ratio\n",
    "    Returns:\n",
    "        dict: Dictionary containing average test losses\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    metric_logger = misc.MetricLogger(delimiter=\"  \")\n",
    "    header = 'Test:'\n",
    "\n",
    "    # Collect all losses for final statistics\n",
    "    all_losses = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for samples, *_ in metric_logger.log_every(data_loader, 20, header):\n",
    "            samples = samples.to(device, non_blocking=True)\n",
    "            \n",
    "            with torch.cuda.amp.autocast():\n",
    "                loss, *_ = model(samples, mask_ratio=0.75)\n",
    "            \n",
    "            loss_value = loss.item()\n",
    "            \n",
    "            if not math.isfinite(loss_value):\n",
    "                print(\"Loss is {}, stopping testing\".format(loss_value))\n",
    "                raise ValueError(f\"Loss is {loss_value}, stopping testing\")\n",
    "           \n",
    "            # Collect losses\n",
    "            all_losses.append(loss_value)\n",
    "            \n",
    "            # Update metrics\n",
    "            metric_logger.update(loss=loss_value)\n",
    "\n",
    "    # Calculate final statistics\n",
    "    avg_loss = sum(all_losses) / len(all_losses)\n",
    "\n",
    "    # Print results\n",
    "    print('=' * 80)\n",
    "    print(f'Test Results:')\n",
    "    print(f'Average Loss: {avg_loss:.4f}')\n",
    "    print('=' * 80)\n",
    "\n",
    "    return {\n",
    "        'test_loss': avg_loss,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "41e23bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = \"/home/ubuntu/satellite-cae/SatMAE/data/test_.csv\"\n",
    "\n",
    "mean = SentinelIndividualImageDataset.mean\n",
    "std = SentinelIndividualImageDataset.std\n",
    "transform = SentinelIndividualImageDataset.build_transform(\n",
    "    is_train=False, input_size=224, mean=mean, std=std\n",
    ")\n",
    "dataset = SentinelIndividualImageDataset(\n",
    "    csv_path=csv_path,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "batch_size = 64\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, drop_last=True, num_workers=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c0d809",
   "metadata": {},
   "source": [
    "# Test CAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1a2ec649",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1624/4194983178.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location='cpu')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['teacher.0.norm1.weight', 'teacher.0.norm1.bias', 'teacher.0.attn.qkv.weight', 'teacher.0.attn.qkv.bias', 'teacher.0.attn.proj.weight', 'teacher.0.attn.proj.bias', 'teacher.0.norm2.weight', 'teacher.0.norm2.bias', 'teacher.0.mlp.fc1.weight', 'teacher.0.mlp.fc1.bias', 'teacher.0.mlp.fc2.weight', 'teacher.0.mlp.fc2.bias', 'teacher.1.norm1.weight', 'teacher.1.norm1.bias', 'teacher.1.attn.qkv.weight', 'teacher.1.attn.qkv.bias', 'teacher.1.attn.proj.weight', 'teacher.1.attn.proj.bias', 'teacher.1.norm2.weight', 'teacher.1.norm2.bias', 'teacher.1.mlp.fc1.weight', 'teacher.1.mlp.fc1.bias', 'teacher.1.mlp.fc2.weight', 'teacher.1.mlp.fc2.bias', 'teacher.2.norm1.weight', 'teacher.2.norm1.bias', 'teacher.2.attn.qkv.weight', 'teacher.2.attn.qkv.bias', 'teacher.2.attn.proj.weight', 'teacher.2.attn.proj.bias', 'teacher.2.norm2.weight', 'teacher.2.norm2.bias', 'teacher.2.mlp.fc1.weight', 'teacher.2.mlp.fc1.bias', 'teacher.2.mlp.fc2.weight', 'teacher.2.mlp.fc2.bias', 'teacher.3.norm1.weight', 'teacher.3.norm1.bias', 'teacher.3.attn.qkv.weight', 'teacher.3.attn.qkv.bias', 'teacher.3.attn.proj.weight', 'teacher.3.attn.proj.bias', 'teacher.3.norm2.weight', 'teacher.3.norm2.bias', 'teacher.3.mlp.fc1.weight', 'teacher.3.mlp.fc1.bias', 'teacher.3.mlp.fc2.weight', 'teacher.3.mlp.fc2.bias', 'teacher.4.norm1.weight', 'teacher.4.norm1.bias', 'teacher.4.attn.qkv.weight', 'teacher.4.attn.qkv.bias', 'teacher.4.attn.proj.weight', 'teacher.4.attn.proj.bias', 'teacher.4.norm2.weight', 'teacher.4.norm2.bias', 'teacher.4.mlp.fc1.weight', 'teacher.4.mlp.fc1.bias', 'teacher.4.mlp.fc2.weight', 'teacher.4.mlp.fc2.bias', 'teacher.5.norm1.weight', 'teacher.5.norm1.bias', 'teacher.5.attn.qkv.weight', 'teacher.5.attn.qkv.bias', 'teacher.5.attn.proj.weight', 'teacher.5.attn.proj.bias', 'teacher.5.norm2.weight', 'teacher.5.norm2.bias', 'teacher.5.mlp.fc1.weight', 'teacher.5.mlp.fc1.bias', 'teacher.5.mlp.fc2.weight', 'teacher.5.mlp.fc2.bias', 'teacher.6.norm1.weight', 'teacher.6.norm1.bias', 'teacher.6.attn.qkv.weight', 'teacher.6.attn.qkv.bias', 'teacher.6.attn.proj.weight', 'teacher.6.attn.proj.bias', 'teacher.6.norm2.weight', 'teacher.6.norm2.bias', 'teacher.6.mlp.fc1.weight', 'teacher.6.mlp.fc1.bias', 'teacher.6.mlp.fc2.weight', 'teacher.6.mlp.fc2.bias', 'teacher.7.norm1.weight', 'teacher.7.norm1.bias', 'teacher.7.attn.qkv.weight', 'teacher.7.attn.qkv.bias', 'teacher.7.attn.proj.weight', 'teacher.7.attn.proj.bias', 'teacher.7.norm2.weight', 'teacher.7.norm2.bias', 'teacher.7.mlp.fc1.weight', 'teacher.7.mlp.fc1.bias', 'teacher.7.mlp.fc2.weight', 'teacher.7.mlp.fc2.bias', 'teacher.8.norm1.weight', 'teacher.8.norm1.bias', 'teacher.8.attn.qkv.weight', 'teacher.8.attn.qkv.bias', 'teacher.8.attn.proj.weight', 'teacher.8.attn.proj.bias', 'teacher.8.norm2.weight', 'teacher.8.norm2.bias', 'teacher.8.mlp.fc1.weight', 'teacher.8.mlp.fc1.bias', 'teacher.8.mlp.fc2.weight', 'teacher.8.mlp.fc2.bias', 'teacher.9.norm1.weight', 'teacher.9.norm1.bias', 'teacher.9.attn.qkv.weight', 'teacher.9.attn.qkv.bias', 'teacher.9.attn.proj.weight', 'teacher.9.attn.proj.bias', 'teacher.9.norm2.weight', 'teacher.9.norm2.bias', 'teacher.9.mlp.fc1.weight', 'teacher.9.mlp.fc1.bias', 'teacher.9.mlp.fc2.weight', 'teacher.9.mlp.fc2.bias', 'teacher.10.norm1.weight', 'teacher.10.norm1.bias', 'teacher.10.attn.qkv.weight', 'teacher.10.attn.qkv.bias', 'teacher.10.attn.proj.weight', 'teacher.10.attn.proj.bias', 'teacher.10.norm2.weight', 'teacher.10.norm2.bias', 'teacher.10.mlp.fc1.weight', 'teacher.10.mlp.fc1.bias', 'teacher.10.mlp.fc2.weight', 'teacher.10.mlp.fc2.bias', 'teacher.11.norm1.weight', 'teacher.11.norm1.bias', 'teacher.11.attn.qkv.weight', 'teacher.11.attn.qkv.bias', 'teacher.11.attn.proj.weight', 'teacher.11.attn.proj.bias', 'teacher.11.norm2.weight', 'teacher.11.norm2.bias', 'teacher.11.mlp.fc1.weight', 'teacher.11.mlp.fc1.bias', 'teacher.11.mlp.fc2.weight', 'teacher.11.mlp.fc2.bias', 'regressor_norm.weight', 'regressor_norm.bias'], unexpected_keys=[])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cae = cae_vit_base_patch16(in_chans=13)\n",
    "cae.to(device)\n",
    "\n",
    "checkpoint_path = \"/home/ubuntu/checkpoint-98.pth\"\n",
    "# checkpoint_path = \"/home/ubuntu/satellite-cae/SatMAE/output_dir/checkpoint-0.pth\"\n",
    "checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "cae.load_state_dict(checkpoint['model'], strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d4413a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1624/1190585970.py:28: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test:  [   0/1327]  eta: 4:27:53  loss: 8.6131 (8.6131)  loss_main: 0.9407 (0.9407)  loss_align: 7.6724 (7.6724)  time: 12.1125  data: 11.9344  max mem: 5627\n",
      "Test:  [  20/1327]  eta: 0:18:36  loss: 67.2100 (63.9049)  loss_main: 1.0811 (1.0624)  loss_align: 66.0713 (62.8425)  time: 0.2911  data: 0.1202  max mem: 5627\n",
      "Test:  [  40/1327]  eta: 0:12:36  loss: 67.6484 (65.6110)  loss_main: 1.0833 (1.0756)  loss_align: 66.5650 (64.5353)  time: 0.3080  data: 0.1357  max mem: 5627\n",
      "Test:  [  60/1327]  eta: 0:10:23  loss: 68.3950 (66.8223)  loss_main: 0.8738 (1.0081)  loss_align: 67.4525 (65.8143)  time: 0.2956  data: 0.1273  max mem: 5627\n",
      "Test:  [  80/1327]  eta: 0:09:15  loss: 68.7400 (67.2066)  loss_main: 0.9595 (1.0099)  loss_align: 67.6276 (66.1967)  time: 0.3026  data: 0.1400  max mem: 5627\n",
      "Test:  [ 100/1327]  eta: 0:08:31  loss: 67.1465 (67.3012)  loss_main: 1.0416 (1.0188)  loss_align: 66.1099 (66.2824)  time: 0.3038  data: 0.1410  max mem: 5627\n",
      "Test:  [ 120/1327]  eta: 0:07:58  loss: 66.6422 (67.2470)  loss_main: 1.0497 (1.0260)  loss_align: 65.6051 (66.2211)  time: 0.2931  data: 0.1299  max mem: 5627\n",
      "Test:  [ 140/1327]  eta: 0:07:38  loss: 68.5861 (67.4518)  loss_main: 1.0465 (1.0292)  loss_align: 67.4140 (66.4226)  time: 0.3214  data: 0.1572  max mem: 5627\n",
      "Test:  [ 160/1327]  eta: 0:07:17  loss: 72.3472 (68.0289)  loss_main: 0.9825 (1.0243)  loss_align: 71.3277 (67.0045)  time: 0.2940  data: 0.1297  max mem: 5627\n",
      "Test:  [ 180/1327]  eta: 0:07:04  loss: 66.8253 (67.9818)  loss_main: 1.0592 (1.0273)  loss_align: 65.7464 (66.9545)  time: 0.3364  data: 0.1746  max mem: 5627\n",
      "Test:  [ 200/1327]  eta: 0:06:48  loss: 65.3158 (67.7158)  loss_main: 1.0781 (1.0321)  loss_align: 64.2615 (66.6837)  time: 0.2879  data: 0.1255  max mem: 5627\n",
      "Test:  [ 220/1327]  eta: 0:06:37  loss: 64.9227 (67.4638)  loss_main: 1.0695 (1.0358)  loss_align: 63.8504 (66.4280)  time: 0.3315  data: 0.1686  max mem: 5627\n",
      "Test:  [ 240/1327]  eta: 0:06:24  loss: 65.1578 (67.2990)  loss_main: 1.0708 (1.0387)  loss_align: 64.0781 (66.2602)  time: 0.2897  data: 0.1275  max mem: 5627\n",
      "Test:  [ 260/1327]  eta: 0:06:14  loss: 65.4097 (67.1587)  loss_main: 1.0711 (1.0427)  loss_align: 64.3255 (66.1160)  time: 0.3206  data: 0.1587  max mem: 5627\n",
      "Test:  [ 280/1327]  eta: 0:06:03  loss: 67.2565 (67.1356)  loss_main: 1.1059 (1.0462)  loss_align: 66.1405 (66.0894)  time: 0.2951  data: 0.1315  max mem: 5627\n",
      "Test:  [ 300/1327]  eta: 0:05:54  loss: 71.3491 (67.3847)  loss_main: 1.0193 (1.0454)  loss_align: 70.3891 (66.3394)  time: 0.3128  data: 0.1500  max mem: 5627\n",
      "Test:  [ 320/1327]  eta: 0:05:44  loss: 71.1750 (67.6206)  loss_main: 0.9867 (1.0417)  loss_align: 70.1592 (66.5789)  time: 0.2957  data: 0.1323  max mem: 5627\n",
      "Test:  [ 340/1327]  eta: 0:05:35  loss: 70.3129 (67.8000)  loss_main: 0.9992 (1.0399)  loss_align: 69.3302 (66.7601)  time: 0.3176  data: 0.1537  max mem: 5627\n",
      "Test:  [ 360/1327]  eta: 0:05:27  loss: 69.6279 (67.9009)  loss_main: 1.0666 (1.0419)  loss_align: 68.5407 (66.8590)  time: 0.3027  data: 0.1387  max mem: 5627\n",
      "Test:  [ 380/1327]  eta: 0:05:18  loss: 70.4811 (68.0605)  loss_main: 1.0117 (1.0404)  loss_align: 69.4399 (67.0201)  time: 0.3088  data: 0.1461  max mem: 5627\n",
      "Test:  [ 400/1327]  eta: 0:05:10  loss: 70.2390 (68.1284)  loss_main: 1.0333 (1.0398)  loss_align: 69.1790 (67.0886)  time: 0.2929  data: 0.1287  max mem: 5627\n",
      "Test:  [ 420/1327]  eta: 0:05:02  loss: 70.4876 (68.2506)  loss_main: 1.0009 (1.0379)  loss_align: 69.4812 (67.2127)  time: 0.3102  data: 0.1472  max mem: 5627\n",
      "Test:  [ 440/1327]  eta: 0:04:54  loss: 69.9623 (68.3288)  loss_main: 1.0046 (1.0365)  loss_align: 68.9472 (67.2923)  time: 0.3035  data: 0.1394  max mem: 5627\n",
      "Test:  [ 460/1327]  eta: 0:04:47  loss: 68.5653 (68.3487)  loss_main: 0.9910 (1.0343)  loss_align: 67.6269 (67.3144)  time: 0.3115  data: 0.1479  max mem: 5627\n",
      "Test:  [ 480/1327]  eta: 0:04:39  loss: 70.4614 (68.4286)  loss_main: 1.0186 (1.0336)  loss_align: 69.4421 (67.3951)  time: 0.3004  data: 0.1364  max mem: 5627\n",
      "Test:  [ 500/1327]  eta: 0:04:32  loss: 69.6565 (68.4668)  loss_main: 1.0204 (1.0336)  loss_align: 68.6081 (67.4332)  time: 0.3074  data: 0.1450  max mem: 5627\n",
      "Test:  [ 520/1327]  eta: 0:04:24  loss: 69.0362 (68.4972)  loss_main: 1.0047 (1.0333)  loss_align: 68.1013 (67.4639)  time: 0.3061  data: 0.1418  max mem: 5627\n",
      "Test:  [ 540/1327]  eta: 0:04:18  loss: 71.9319 (68.6302)  loss_main: 0.9774 (1.0312)  loss_align: 70.8890 (67.5990)  time: 0.3262  data: 0.1640  max mem: 5627\n",
      "Test:  [ 560/1327]  eta: 0:04:10  loss: 67.9742 (68.6716)  loss_main: 1.1699 (1.0353)  loss_align: 66.7671 (67.6363)  time: 0.2905  data: 0.1268  max mem: 5627\n",
      "Test:  [ 580/1327]  eta: 0:04:04  loss: 66.9391 (68.6376)  loss_main: 1.1602 (1.0408)  loss_align: 65.7914 (67.5968)  time: 0.3363  data: 0.1731  max mem: 5627\n",
      "Test:  [ 600/1327]  eta: 0:03:56  loss: 68.1211 (68.6260)  loss_main: 1.1388 (1.0449)  loss_align: 66.9880 (67.5811)  time: 0.2892  data: 0.1251  max mem: 5627\n",
      "Test:  [ 620/1327]  eta: 0:03:54  loss: 69.2481 (68.6293)  loss_main: 1.0911 (1.0458)  loss_align: 68.1511 (67.5835)  time: 0.4892  data: 0.3253  max mem: 5627\n",
      "Test:  [ 640/1327]  eta: 0:03:46  loss: 70.8329 (68.7025)  loss_main: 1.0148 (1.0448)  loss_align: 69.8088 (67.6577)  time: 0.2666  data: 0.1047  max mem: 5627\n",
      "Test:  [ 660/1327]  eta: 0:03:39  loss: 66.4265 (68.6393)  loss_main: 1.2061 (1.0500)  loss_align: 65.2491 (67.5892)  time: 0.3284  data: 0.1658  max mem: 5627\n",
      "Test:  [ 680/1327]  eta: 0:03:32  loss: 70.1875 (68.6727)  loss_main: 1.0069 (1.0496)  loss_align: 69.1891 (67.6232)  time: 0.2912  data: 0.1281  max mem: 5627\n",
      "Test:  [ 700/1327]  eta: 0:03:25  loss: 72.4852 (68.7713)  loss_main: 0.9998 (1.0481)  loss_align: 71.4949 (67.7232)  time: 0.3246  data: 0.1630  max mem: 5627\n",
      "Test:  [ 720/1327]  eta: 0:03:18  loss: 72.1582 (68.8485)  loss_main: 0.9893 (1.0464)  loss_align: 71.1749 (67.8022)  time: 0.2867  data: 0.1229  max mem: 5627\n",
      "Test:  [ 740/1327]  eta: 0:03:11  loss: 67.6099 (68.8301)  loss_main: 1.0185 (1.0460)  loss_align: 66.6369 (67.7841)  time: 0.3341  data: 0.1718  max mem: 5627\n",
      "Test:  [ 760/1327]  eta: 0:03:04  loss: 68.7355 (68.8187)  loss_main: 1.0156 (1.0456)  loss_align: 67.6654 (67.7731)  time: 0.2896  data: 0.1266  max mem: 5627\n",
      "Test:  [ 780/1327]  eta: 0:02:58  loss: 67.6947 (68.8097)  loss_main: 1.0465 (1.0459)  loss_align: 66.6643 (67.7638)  time: 0.3270  data: 0.1633  max mem: 5627\n",
      "Test:  [ 800/1327]  eta: 0:02:51  loss: 68.2896 (68.8078)  loss_main: 1.0624 (1.0464)  loss_align: 67.2137 (67.7614)  time: 0.2886  data: 0.1249  max mem: 5627\n",
      "Test:  [ 820/1327]  eta: 0:02:50  loss: 69.3915 (68.8187)  loss_main: 0.9650 (1.0427)  loss_align: 68.4043 (67.7759)  time: 0.7943  data: 0.6204  max mem: 5627\n",
      "Test:  [ 840/1327]  eta: 0:02:43  loss: 67.7067 (68.8134)  loss_main: 1.0429 (1.0430)  loss_align: 66.7041 (67.7704)  time: 0.2797  data: 0.1174  max mem: 5627\n",
      "Test:  [ 860/1327]  eta: 0:02:36  loss: 70.5226 (68.8487)  loss_main: 1.0539 (1.0428)  loss_align: 69.4514 (67.8058)  time: 0.3279  data: 0.1653  max mem: 5627\n",
      "Test:  [ 880/1327]  eta: 0:02:29  loss: 70.9536 (68.8989)  loss_main: 1.0095 (1.0420)  loss_align: 69.9577 (67.8569)  time: 0.2885  data: 0.1259  max mem: 5627\n",
      "Test:  [ 900/1327]  eta: 0:02:22  loss: 69.3646 (68.9001)  loss_main: 1.0099 (1.0414)  loss_align: 68.2992 (67.8587)  time: 0.3330  data: 0.1696  max mem: 5627\n",
      "Test:  [ 920/1327]  eta: 0:02:15  loss: 69.7618 (68.9279)  loss_main: 0.9971 (1.0405)  loss_align: 68.7263 (67.8873)  time: 0.2820  data: 0.1192  max mem: 5627\n",
      "Test:  [ 940/1327]  eta: 0:02:08  loss: 68.3631 (68.9161)  loss_main: 1.0094 (1.0399)  loss_align: 67.3581 (67.8762)  time: 0.3152  data: 0.1528  max mem: 5627\n",
      "Test:  [ 960/1327]  eta: 0:02:01  loss: 68.2996 (68.9192)  loss_main: 1.0117 (1.0394)  loss_align: 67.2847 (67.8798)  time: 0.2852  data: 0.1221  max mem: 5627\n",
      "Test:  [ 980/1327]  eta: 0:01:55  loss: 68.9062 (68.9224)  loss_main: 1.0202 (1.0392)  loss_align: 67.8942 (67.8832)  time: 0.3435  data: 0.1745  max mem: 5627\n",
      "Test:  [1000/1327]  eta: 0:01:49  loss: 69.7926 (68.9378)  loss_main: 1.0138 (1.0379)  loss_align: 68.7455 (67.8999)  time: 0.5032  data: 0.3414  max mem: 5627\n",
      "Test:  [1020/1327]  eta: 0:01:42  loss: 64.7967 (68.8670)  loss_main: 1.1500 (1.0399)  loss_align: 63.6693 (67.8270)  time: 0.2852  data: 0.1225  max mem: 5627\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test:  [1040/1327]  eta: 0:01:35  loss: 64.8262 (68.8015)  loss_main: 1.1994 (1.0428)  loss_align: 63.6579 (67.7587)  time: 0.3244  data: 0.1612  max mem: 5627\n",
      "Test:  [1060/1327]  eta: 0:01:28  loss: 70.2621 (68.8271)  loss_main: 1.0239 (1.0425)  loss_align: 69.2353 (67.7845)  time: 0.2848  data: 0.1211  max mem: 5627\n",
      "Test:  [1080/1327]  eta: 0:01:22  loss: 70.0705 (68.8591)  loss_main: 1.0603 (1.0429)  loss_align: 68.9998 (67.8163)  time: 0.3219  data: 0.1595  max mem: 5627\n",
      "Test:  [1100/1327]  eta: 0:01:15  loss: 69.3203 (68.8660)  loss_main: 1.0754 (1.0434)  loss_align: 68.2572 (67.8226)  time: 0.2823  data: 0.1186  max mem: 5627\n",
      "Test:  [1120/1327]  eta: 0:01:08  loss: 69.5305 (68.8760)  loss_main: 1.0151 (1.0430)  loss_align: 68.5643 (67.8331)  time: 0.3322  data: 0.1682  max mem: 5627\n",
      "Test:  [1140/1327]  eta: 0:01:01  loss: 68.1279 (68.8737)  loss_main: 1.0642 (1.0434)  loss_align: 67.0311 (67.8303)  time: 0.2839  data: 0.1208  max mem: 5627\n",
      "Test:  [1160/1327]  eta: 0:00:55  loss: 68.6550 (68.8670)  loss_main: 1.1034 (1.0444)  loss_align: 67.5787 (67.8226)  time: 0.3224  data: 0.1586  max mem: 5627\n",
      "Test:  [1180/1327]  eta: 0:00:48  loss: 65.7634 (68.8138)  loss_main: 1.0006 (1.0437)  loss_align: 64.7583 (67.7701)  time: 0.2835  data: 0.1207  max mem: 5627\n",
      "Test:  [1200/1327]  eta: 0:00:41  loss: 70.5710 (68.8357)  loss_main: 1.0165 (1.0432)  loss_align: 69.5663 (67.7925)  time: 0.3243  data: 0.1616  max mem: 5627\n",
      "Test:  [1220/1327]  eta: 0:00:35  loss: 68.2935 (68.8379)  loss_main: 1.0761 (1.0439)  loss_align: 67.2560 (67.7941)  time: 0.2891  data: 0.1266  max mem: 5627\n",
      "Test:  [1240/1327]  eta: 0:00:28  loss: 70.4384 (68.8750)  loss_main: 1.0157 (1.0437)  loss_align: 69.3652 (67.8312)  time: 0.3247  data: 0.1623  max mem: 5627\n",
      "Test:  [1260/1327]  eta: 0:00:22  loss: 71.5047 (68.9115)  loss_main: 0.9984 (1.0431)  loss_align: 70.5087 (67.8684)  time: 0.2875  data: 0.1242  max mem: 5627\n",
      "Test:  [1280/1327]  eta: 0:00:15  loss: 69.5775 (68.9221)  loss_main: 1.0215 (1.0428)  loss_align: 68.5693 (67.8793)  time: 0.3237  data: 0.1611  max mem: 5627\n",
      "Test:  [1300/1327]  eta: 0:00:08  loss: 68.2003 (68.9152)  loss_main: 1.0611 (1.0435)  loss_align: 67.1920 (67.8717)  time: 0.2872  data: 0.1239  max mem: 5627\n",
      "Test:  [1320/1327]  eta: 0:00:02  loss: 70.1483 (68.9294)  loss_main: 1.1657 (1.0454)  loss_align: 69.0086 (67.8839)  time: 0.2936  data: 0.1337  max mem: 5627\n",
      "Test:  [1326/1327]  eta: 0:00:00  loss: 70.4962 (68.9323)  loss_main: 1.1383 (1.0453)  loss_align: 69.1998 (67.8870)  time: 0.2525  data: 0.0953  max mem: 5627\n",
      "Test: Total time: 0:07:13 (0.3269 s / it)\n",
      "================================================================================\n",
      "Test Results:\n",
      "Average Loss: 68.9323\n",
      "Average Main Loss: 1.0453\n",
      "Average Align Loss: 67.8870\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_loss': 68.93226244005837,\n",
       " 'test_loss_main': 1.045291825713479,\n",
       " 'test_loss_align': 67.88697067956414}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(cae, dataloader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb09e37",
   "metadata": {},
   "source": [
    "# Test MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b4546529",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1624/1869589092.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location='cpu')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mae = mae_vit_base_patch16(in_chans=13)\n",
    "mae.to(device)\n",
    "\n",
    "checkpoint_path = \"/home/ubuntu/checkpoint-100.pth\"\n",
    "# checkpoint_path = \"/home/ubuntu/satellite-cae/SatMAE/output_dir/checkpoint-satmae-99.pth\"\n",
    "checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "mae.load_state_dict(checkpoint['model'], strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "67409877",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1624/3645175985.py:97: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test:  [   0/1327]  eta: 4:25:27  loss: 0.9452 (0.9452)  time: 12.0028  data: 11.9024  max mem: 5627\n",
      "Test:  [  20/1327]  eta: 0:18:24  loss: 1.0780 (1.0610)  time: 0.2873  data: 0.1954  max mem: 5627\n",
      "Test:  [  40/1327]  eta: 0:12:43  loss: 1.0798 (1.0739)  time: 0.3280  data: 0.2309  max mem: 5627\n",
      "Test:  [  60/1327]  eta: 0:10:26  loss: 0.8634 (1.0035)  time: 0.2919  data: 0.1893  max mem: 5627\n",
      "Test:  [  80/1327]  eta: 0:09:16  loss: 0.9618 (1.0064)  time: 0.2994  data: 0.2095  max mem: 5627\n",
      "Test:  [ 100/1327]  eta: 0:08:38  loss: 1.0424 (1.0161)  time: 0.3275  data: 0.2358  max mem: 5627\n",
      "Test:  [ 120/1327]  eta: 0:07:59  loss: 1.0493 (1.0238)  time: 0.2696  data: 0.1770  max mem: 5627\n",
      "Test:  [ 140/1327]  eta: 0:07:41  loss: 1.0450 (1.0274)  time: 0.3400  data: 0.2518  max mem: 5627\n",
      "Test:  [ 160/1327]  eta: 0:07:17  loss: 0.9753 (1.0217)  time: 0.2772  data: 0.1895  max mem: 5627\n",
      "Test:  [ 180/1327]  eta: 0:07:07  loss: 1.0610 (1.0249)  time: 0.3536  data: 0.2676  max mem: 5627\n",
      "Test:  [ 200/1327]  eta: 0:06:49  loss: 1.0786 (1.0300)  time: 0.2754  data: 0.1892  max mem: 5627\n",
      "Test:  [ 220/1327]  eta: 0:06:40  loss: 1.0726 (1.0341)  time: 0.3459  data: 0.2552  max mem: 5627\n",
      "Test:  [ 240/1327]  eta: 0:06:25  loss: 1.0723 (1.0374)  time: 0.2754  data: 0.1865  max mem: 5627\n",
      "Test:  [ 260/1327]  eta: 0:06:15  loss: 1.0722 (1.0417)  time: 0.3204  data: 0.2285  max mem: 5627\n",
      "Test:  [ 280/1327]  eta: 0:06:03  loss: 1.1123 (1.0455)  time: 0.2835  data: 0.1878  max mem: 5627\n",
      "Test:  [ 300/1327]  eta: 0:05:55  loss: 1.0188 (1.0446)  time: 0.3344  data: 0.2444  max mem: 5627\n",
      "Test:  [ 320/1327]  eta: 0:05:44  loss: 0.9780 (1.0407)  time: 0.2766  data: 0.1904  max mem: 5627\n",
      "Test:  [ 340/1327]  eta: 0:05:37  loss: 0.9930 (1.0387)  time: 0.3368  data: 0.2451  max mem: 5627\n",
      "Test:  [ 360/1327]  eta: 0:05:27  loss: 1.0649 (1.0406)  time: 0.2812  data: 0.1920  max mem: 5627\n",
      "Test:  [ 380/1327]  eta: 0:05:19  loss: 1.0069 (1.0390)  time: 0.3232  data: 0.2331  max mem: 5627\n",
      "Test:  [ 400/1327]  eta: 0:05:09  loss: 1.0253 (1.0383)  time: 0.2712  data: 0.1821  max mem: 5627\n",
      "Test:  [ 420/1327]  eta: 0:05:03  loss: 0.9956 (1.0362)  time: 0.3450  data: 0.2522  max mem: 5627\n",
      "Test:  [ 440/1327]  eta: 0:04:54  loss: 0.9975 (1.0346)  time: 0.2744  data: 0.1826  max mem: 5627\n",
      "Test:  [ 460/1327]  eta: 0:04:48  loss: 0.9864 (1.0323)  time: 0.3390  data: 0.2530  max mem: 5627\n",
      "Test:  [ 480/1327]  eta: 0:04:39  loss: 1.0097 (1.0314)  time: 0.2678  data: 0.1794  max mem: 5627\n",
      "Test:  [ 500/1327]  eta: 0:04:32  loss: 1.0164 (1.0314)  time: 0.3311  data: 0.2445  max mem: 5627\n",
      "Test:  [ 520/1327]  eta: 0:04:24  loss: 1.0053 (1.0309)  time: 0.2792  data: 0.1927  max mem: 5627\n",
      "Test:  [ 540/1327]  eta: 0:04:18  loss: 0.9825 (1.0288)  time: 0.3338  data: 0.2476  max mem: 5627\n",
      "Test:  [ 560/1327]  eta: 0:04:09  loss: 1.1713 (1.0330)  time: 0.2707  data: 0.1815  max mem: 5627\n",
      "Test:  [ 580/1327]  eta: 0:04:03  loss: 1.1621 (1.0386)  time: 0.3268  data: 0.2355  max mem: 5627\n",
      "Test:  [ 600/1327]  eta: 0:03:55  loss: 1.1361 (1.0427)  time: 0.2697  data: 0.1833  max mem: 5627\n",
      "Test:  [ 620/1327]  eta: 0:03:54  loss: 1.0845 (1.0434)  time: 0.5465  data: 0.4589  max mem: 5627\n",
      "Test:  [ 640/1327]  eta: 0:03:45  loss: 1.0065 (1.0424)  time: 0.2592  data: 0.1755  max mem: 5627\n",
      "Test:  [ 660/1327]  eta: 0:03:39  loss: 1.2101 (1.0477)  time: 0.3306  data: 0.2412  max mem: 5627\n",
      "Test:  [ 680/1327]  eta: 0:03:31  loss: 0.9975 (1.0472)  time: 0.2745  data: 0.1841  max mem: 5627\n",
      "Test:  [ 700/1327]  eta: 0:03:25  loss: 0.9943 (1.0456)  time: 0.3381  data: 0.2487  max mem: 5627\n",
      "Test:  [ 720/1327]  eta: 0:03:17  loss: 0.9804 (1.0438)  time: 0.2723  data: 0.1844  max mem: 5627\n",
      "Test:  [ 740/1327]  eta: 0:03:11  loss: 1.0176 (1.0434)  time: 0.3368  data: 0.2486  max mem: 5627\n",
      "Test:  [ 760/1327]  eta: 0:03:04  loss: 1.0154 (1.0431)  time: 0.2591  data: 0.1769  max mem: 5627\n",
      "Test:  [ 780/1327]  eta: 0:02:57  loss: 1.0379 (1.0433)  time: 0.3401  data: 0.2505  max mem: 5627\n",
      "Test:  [ 800/1327]  eta: 0:02:50  loss: 1.0633 (1.0438)  time: 0.2675  data: 0.1821  max mem: 5627\n",
      "Test:  [ 820/1327]  eta: 0:02:49  loss: 0.9548 (1.0400)  time: 0.7959  data: 0.6981  max mem: 5627\n",
      "Test:  [ 840/1327]  eta: 0:02:42  loss: 1.0391 (1.0402)  time: 0.2660  data: 0.1796  max mem: 5627\n",
      "Test:  [ 860/1327]  eta: 0:02:35  loss: 1.0531 (1.0401)  time: 0.3176  data: 0.2306  max mem: 5627\n",
      "Test:  [ 880/1327]  eta: 0:02:28  loss: 1.0040 (1.0392)  time: 0.2680  data: 0.1766  max mem: 5627\n",
      "Test:  [ 900/1327]  eta: 0:02:21  loss: 1.0026 (1.0386)  time: 0.3479  data: 0.2597  max mem: 5627\n",
      "Test:  [ 920/1327]  eta: 0:02:14  loss: 0.9963 (1.0377)  time: 0.2717  data: 0.1824  max mem: 5627\n",
      "Test:  [ 940/1327]  eta: 0:02:08  loss: 1.0054 (1.0371)  time: 0.3341  data: 0.2429  max mem: 5627\n",
      "Test:  [ 960/1327]  eta: 0:02:00  loss: 1.0122 (1.0366)  time: 0.2561  data: 0.1649  max mem: 5627\n",
      "Test:  [ 980/1327]  eta: 0:01:54  loss: 1.0177 (1.0363)  time: 0.3589  data: 0.2649  max mem: 5627\n",
      "Test:  [1000/1327]  eta: 0:01:48  loss: 1.0019 (1.0350)  time: 0.4998  data: 0.4114  max mem: 5627\n",
      "Test:  [1020/1327]  eta: 0:01:41  loss: 1.1479 (1.0371)  time: 0.2595  data: 0.1708  max mem: 5627\n",
      "Test:  [1040/1327]  eta: 0:01:35  loss: 1.1973 (1.0399)  time: 0.3329  data: 0.2453  max mem: 5627\n",
      "Test:  [1060/1327]  eta: 0:01:28  loss: 1.0176 (1.0396)  time: 0.2726  data: 0.1817  max mem: 5627\n",
      "Test:  [1080/1327]  eta: 0:01:21  loss: 1.0584 (1.0399)  time: 0.3280  data: 0.2442  max mem: 5627\n",
      "Test:  [1100/1327]  eta: 0:01:14  loss: 1.0741 (1.0406)  time: 0.2653  data: 0.1779  max mem: 5627\n",
      "Test:  [1120/1327]  eta: 0:01:08  loss: 1.0095 (1.0400)  time: 0.3312  data: 0.2434  max mem: 5627\n",
      "Test:  [1140/1327]  eta: 0:01:01  loss: 1.0614 (1.0405)  time: 0.2663  data: 0.1822  max mem: 5627\n",
      "Test:  [1160/1327]  eta: 0:00:54  loss: 1.1020 (1.0415)  time: 0.3426  data: 0.2512  max mem: 5627\n",
      "Test:  [1180/1327]  eta: 0:00:48  loss: 0.9989 (1.0409)  time: 0.2734  data: 0.1833  max mem: 5627\n",
      "Test:  [1200/1327]  eta: 0:00:41  loss: 1.0181 (1.0403)  time: 0.3299  data: 0.2409  max mem: 5627\n",
      "Test:  [1220/1327]  eta: 0:00:34  loss: 1.0755 (1.0411)  time: 0.2656  data: 0.1767  max mem: 5627\n",
      "Test:  [1240/1327]  eta: 0:00:28  loss: 1.0148 (1.0410)  time: 0.3422  data: 0.2532  max mem: 5627\n",
      "Test:  [1260/1327]  eta: 0:00:21  loss: 0.9993 (1.0404)  time: 0.2632  data: 0.1751  max mem: 5627\n",
      "Test:  [1280/1327]  eta: 0:00:15  loss: 1.0205 (1.0400)  time: 0.3494  data: 0.2568  max mem: 5627\n",
      "Test:  [1300/1327]  eta: 0:00:08  loss: 1.0619 (1.0408)  time: 0.2612  data: 0.1739  max mem: 5627\n",
      "Test:  [1320/1327]  eta: 0:00:02  loss: 1.1678 (1.0428)  time: 0.2988  data: 0.2157  max mem: 5627\n",
      "Test:  [1326/1327]  eta: 0:00:00  loss: 1.1406 (1.0427)  time: 0.2314  data: 0.1536  max mem: 5627\n",
      "Test: Total time: 0:07:10 (0.3242 s / it)\n",
      "================================================================================\n",
      "Test Results:\n",
      "Average Loss: 1.0427\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_loss': 1.0426678094274582}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_mae(mae, dataloader, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
